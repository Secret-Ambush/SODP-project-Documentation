{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Hello there \ud83d\ude00","text":"<p>This is the documentation for my models and the thought process behind selection and evaluation.</p>"},{"location":"#models-used","title":"Models used","text":"<ul> <li><code>XGBoost Classifier Model</code> - Predicting 'yes/no' depending on whether the flight will get full 30 days prior</li> <li><code>XGBoost Regression Model</code> - Predicting when the flight will get full</li> </ul>"},{"location":"#interface","title":"Interface","text":"<ul> <li><code>Streamlit</code> - A Python based dashboard builder</li> <li><code>Plotly</code>, <code>Calplot</code> - Python lib for interactive visualisations</li> </ul>"},{"location":"#contents-in-this-web-page","title":"Contents in this web page","text":"<ul> <li>Classifier Model Documentation</li> <li>Regression Model and Visualisations Documentation</li> <li>Interface building</li> </ul>"},{"location":"Classification%20Model%20Documentation/","title":"Classification Model","text":""},{"location":"Classification%20Model%20Documentation/#section-1-handling-data-set-preparing-and-preprocessing","title":"Section 1: Handling Data Set \u2013 Preparing and Preprocessing","text":"<p>In this project, I had the opportunity to work with an extensive dataset comprising an impressive 500 million rows, which far surpassed any dataset I had handled before. To provide a perspective, the most recognizable Titanic dataset consists of approximately 1000 records, emphasizing the scale of the data I managed.</p> <p>For training the predictive models, I utilized historical flight data spanning the years 2017-2018, and to evaluate their performance, I employed the data from the year 2019. This division into training and testing datasets allowed me to ensure the robustness and accuracy of the models in real-world scenarios.</p> <p>Handling such a vast amount of data was an exciting challenge, and it provided valuable insights into the complexities and nuances of the airline industry's demand patterns. As an intern, I was given an 'x-small' workspace on the cloud data platform 'Snowflake', my line manager tried to up my permissions, but to no avail.</p> <p>I attempted to migrate the entire dataset to PostgreSQL, leaving my laptop overnight at the office for the task, but 500 million rows proved to be too much for PostgreSQL to handle.</p> <p>Eventually, I resorted to using Snowflake programmatically, extracting data at the flight level. Even then, it was a large amount of data, so I had to perform manipulations on the dataframe level.</p> <p>I carefully extracted relevant features from the dataset and encoded the target variable to represent distinct classes. I recognized that group attributes are particularly important, as their booking and cancellations reflect heavily. I merged the individual and group bookings into one. This way, revenue could be correctly reflected. I performed other pre-processing tasks like standard scaling, frequency encoding, and removing null values.</p>"},{"location":"Classification%20Model%20Documentation/#section-2-model-selection-and-evaluation","title":"Section 2: Model Selection and Evaluation","text":"<p>The first model I worked on aimed to classify whether the flight sells out 30 days prior to departure, which is not desirable for airline companies who sell perishable tickets.</p> <p>I began by selecting a single Flight 0784 LOS-DXB, which had a reasonable amount of data (around 500 thousand records). I performed model selection on this test flight before implementing the process for all 1024 flights.</p>"},{"location":"Classification%20Model%20Documentation/#random-forest-classifier","title":"Random Forest Classifier","text":"<p>First, I tried the Random Forest Classifier. I trained the model on the training data and then predicted the labels for the testing data. The results were promising.</p> <p>To better visualize the model's performance, I utilized a confusion matrix to display the number of true positives, true negatives, false positives, and false negatives.</p>"},{"location":"Classification%20Model%20Documentation/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<p>To optimize the Random Forest Classifier further, I conducted hyperparameter tuning using GridSearchCV and RandomizedSearchCV. GridSearchCV involved exploring various combinations of hyperparameters, such as <code>n_estimators</code>, <code>max_features</code>, <code>max_depth</code>, and <code>max_leaf_nodes</code>, to find the optimal configuration. After identifying the best hyperparameters, I refitted the model and reevaluated its performance, resulting in improved accuracy and an ROC AUC score.</p> <p>Taking a different approach, I employed RandomizedSearchCV for hyperparameter tuning. This technique allowed me to randomly sample hyperparameter values from a predefined parameter grid. After evaluating the model with these randomly selected hyperparameters, I determined the best combination and refitted the model.</p>"},{"location":"Classification%20Model%20Documentation/#exploring-other-models","title":"Exploring Other Models","text":"<p>I also tested a variety of other models, including XGBoost Classifier, Gaussian Naive Bayes, Decision Tree Classifier, and SVM, with a linear kernel. Each model was trained on the training data, and their respective label predictions were made for the testing data.</p> <p>After thorough evaluation, I found XGBoost Classifier to be the best performing model. I performed additional hyperparameter tuning to find the best parameters to apply to my classifier.</p>"},{"location":"Classification%20Model%20Documentation/#model-creation-process","title":"Model Creation Process","text":"<p>I followed the below steps to create models for each flight:</p> <ol> <li> <p>Import the necessary libraries, including pandas, create_engine from sqlalchemy, tqdm for progress tracking, gc for garbage collection, and relevant functions from scikit-learn for evaluation.</p> </li> <li> <p>Establish a connection to the SQL database using the create_engine function from sqlalchemy.</p> </li> <li> <p>Define an SQL query to retrieve distinct flight numbers (fltnum) from the database, sorted in descending order.</p> </li> <li> <p>Set the chunksize, which determines the number of rows to be processed at a time. This helps handle large datasets efficiently.</p> </li> <li> <p>Calculate the total number of rows and chunks to be processed.</p> </li> <li> <p>Initialize a progress bar using tqdm to keep track of the processing progress.</p> </li> <li> <p>Iterate through the chunks obtained from the SQL query and perform the following steps for each chunk:</p> </li> <li> <p>Retrieve data for the specific flight from the database.</p> </li> <li>Preprocess the data, filling NaN values, and calculating a new feature 'sodp' (Sold-out Days-Prior).</li> <li>Determine whether the flight is consistently fully booked, never fully booked, or requires class balancing due to imbalanced data.</li> <li>If the flight requires class balancing, perform oversampling using the <code>resample</code> function to address the imbalance.</li> <li>Encode categorical features using frequency encoding for modeling purposes.</li> <li>Split the data into training and testing sets based on the snapshot date (time-based splitting).</li> <li>Build an XGBoost classifier model and fit it to the training data.</li> <li>Predict the labels for the test data and evaluate the model's performance using ROC AUC score and accuracy metrics.</li> </ol> <p>Overall, the XGBoost Classifier demonstrated superior performance in classifying flight sell-outs, and the data preprocessing steps played a crucial role in ensuring accurate predictions.</p>"},{"location":"Interface%20Building/","title":"Interface Building","text":""},{"location":"Interface%20Building/#overview","title":"Overview","text":"<p>This documentation outlines the development of a Flight Capacity Prediction application using Streamlit. The application provides insights into flight capacity forecasting for a specific airline route and flight number. It offers two main functionalities: a Classification Model to predict whether a flight will be fully booked 30 days prior to its departure date and a Regression Model to predict the exact number of days before the flight is full.</p> <p>The application leverages various components and caching mechanisms to ensure efficient data handling and a seamless user experience.</p>"},{"location":"Interface%20Building/#application-architecture","title":"Application Architecture","text":"<p>The Flight Capacity Prediction application is built using Streamlit, a Python library that simplifies creating web applications for data science and machine learning projects. Streamlit allows developers to turn data scripts into shareable web apps with minimal code, making it an excellent choice for this project.</p>"},{"location":"Interface%20Building/#streamlit-components","title":"Streamlit Components","text":"<p>The application uses different Streamlit components to create a user-friendly interface and interact with the underlying data and models.</p>"},{"location":"Interface%20Building/#sidebar","title":"Sidebar","text":"<p>The application incorporates a sidebar that acts as the primary control panel for the user. It features a title and an image, providing a professional and visually appealing look. Users can select between different functionalities using radio buttons:</p> <ul> <li>Classification Model: Enables users to explore the classification model for predicting full flights 30 days prior to departure.</li> <li>Help: CM: Offers documentation and guidance on using the Classification Model.</li> <li>Regression Model: Allows users to use the regression model to predict the exact number of days before a flight is fully booked.</li> <li>Help: RM: Provides documentation and explanations for the Regression Model.</li> </ul>"},{"location":"Interface%20Building/#classification-model-functionality","title":"Classification Model Functionality","text":""},{"location":"Interface%20Building/#dataset-section","title":"DATASET Section","text":"<p>In the Classification Model section, users are presented with an overview of the dataset. The application reads data from a Snowflake database using the <code>fetch_flight0001_data()</code> function. A loading spinner ensures a smooth experience while the data is being fetched.</p> <p>The dataset is then displayed in a data table, allowing users to get a quick look at the available data. Additionally, users can trigger a Quick Analytics feature that generates and displays a flourish chart showcasing relevant data insights. The chart provides a visual representation of the dataset, helping users grasp patterns and trends effectively.</p>"},{"location":"Interface%20Building/#classifier-model-analytics-section","title":"Classifier Model / Analytics Section","text":"<p>This section provides users with the tools to explore the Classifier Model and analyze flights based on specific criteria. Users can select the origin (lego), destination (legd), and flight number (fltnum) from drop-down menus.</p> <p>Upon selecting these parameters, users can click the \"Generate\" button to see a summary of their choices. The application then processes the data and calculates the SODP (30 days prior) value. The SODP represents the number of days before the flight's departure when it is expected to be full. The result is displayed below the selection inputs, providing users with valuable insights.</p>"},{"location":"Interface%20Building/#regression-model-functionality","title":"Regression Model Functionality","text":"<p>In the Regression Model section, users can predict the exact number of days before a flight is fully booked. The interface allows users to choose the origin (lego), destination (legd), flight number (fltnum), departure date, and days prior using dropdown menus and a slider.</p> <p>The application then processes the selected data using the Regression Model and presents the results visually. Users can see the predicted SODP (days prior) on a calendar plot, which displays the days with the highest booking activity.</p> <p>Furthermore, users can observe the error between the predicted and actual SODP values in a time series plot. This provides an understanding of the model's performance and accuracy in predicting flight capacity.</p>"},{"location":"Interface%20Building/#caching-mechanism","title":"Caching Mechanism","text":"<p>Streamlit offers caching mechanisms to enhance application performance by reducing redundant data processing. The application effectively utilizes caching for two critical functions:</p> <ol> <li> <p><code>@st.cache_resource</code>: This decorator is applied to the <code>get_connection()</code> function, which creates a connection to the Snowflake database. By caching this resource, the application avoids establishing a new connection every time a user interacts with the dataset, optimizing data retrieval.</p> </li> <li> <p><code>@st.cache_data</code>: The <code>fetch_flight_data(flightnumber)</code> function is decorated with <code>@st.cache_data</code> to cache the data retrieved from the database. As data retrieval can be time-consuming, caching this resource ensures that data is fetched only once per user session, significantly improving response times.</p> </li> </ol>"},{"location":"Interface%20Building/#conclusion","title":"Conclusion","text":"<p>The Flight Capacity Prediction application built with Streamlit provides valuable insights into flight capacity forecasting using classification and regression models. By utilizing Streamlit's components and caching mechanisms, the application delivers a user-friendly experience with efficient data handling and visualization. The application's flexibility allows users to explore various flight routes and flight numbers, enabling data-driven decision-making for flight capacity management and optimization.</p>"},{"location":"Regression%20Models%20and%20Visualization/","title":"Regression Models and Visualization","text":"<p>This Markdown documentation explains the purpose and functionality of the provided Python code. The code is designed to build 341 regression models for a sample flight (flight number '0772') by training XGBoost regressors for each day prior to departure. After training the models, the script visualizes the predictions and actual 'SODP' on a calendar heatmap for a particular day prior model.</p>"},{"location":"Regression%20Models%20and%20Visualization/#section-1-setup-and-initialization","title":"Section 1: Setup and Initialization","text":"<p>This section includes the necessary imports and configurations to set up the environment and establish connections to external services and data sources.</p>"},{"location":"Regression%20Models%20and%20Visualization/#importing-libraries","title":"Importing Libraries","text":"<p>The code imports required Python modules, as shown below</p> <p><code>pandas os sqlalchemy snowflake pickle matplotlib numpy tqdm xgboost concurrent.futures plotly.graph_objects</code></p>"},{"location":"Regression%20Models%20and%20Visualization/#connecting-to-snowflake","title":"#### Connecting to Snowflake","text":"<p>The script uses SQLAlchemy and Snowflake libraries to connect to a Snowflake database. The connection details are fetched from environment variables ('snowflake_conn').</p>"},{"location":"Regression%20Models%20and%20Visualization/#removing-proxy-variables","title":"Removing Proxy Variables","text":"<p>Two proxy variables ('HTTP_PROXY' and 'HTTPS_PROXY') are removed from the environment variables to ensure that local proxy settings are not used.</p>"},{"location":"Regression%20Models%20and%20Visualization/#creating-sqlalchemy-engine","title":"Creating SQLAlchemy Engine","text":"<p>An instance of sqlalchemy.engine is created using the Snowflake connection details.</p>"},{"location":"Regression%20Models%20and%20Visualization/#section-2-data-preprocessing-and-feature-engineering","title":"Section 2: Data Preprocessing and Feature Engineering","text":"<p>This section consists of data preprocessing and feature engineering steps. The primary DataFrame 'flight' likely contains flight-related data with various columns like 'fltdep' (Flight Departure Date), 'days_prior' (Days Prior to the Flight), 'sodp' (Snap-On Date Prior), 'bkd' (Bookings), 'cap' (Capacity), 'rev' (Revenue), and 'frc_unc' (Uncertain Forecast).</p>"},{"location":"Regression%20Models%20and%20Visualization/#preparing-dataframe-and-feature-engineering","title":"Preparing DataFrame and Feature Engineering","text":"<p>The variable <code>distinct_flights</code> is assigned the same DataFrame 'flight', which will be used to process the data for model training and testing.</p>"},{"location":"Regression%20Models%20and%20Visualization/#finding-maximum-sodp-and-updating-sodp","title":"Finding Maximum 'sodp' and Updating 'sodp'","text":"<p>The code groups the data by 'fltdep' and finds the maximum 'sodp' value for each group. The 'sodp' column is then updated with the maximum 'sodp' value for each respective 'fltdep' group. The 'max_sodp' column is dropped if not needed.</p>"},{"location":"Regression%20Models%20and%20Visualization/#filtering-and-sorting-data-for-model-training","title":"Filtering and Sorting Data for Model Training","text":"<p>The code filters the data to include rows where 'days_prior' is greater than or equal to 100 and 'fltdep' is less than '2019-01-01' (target_date). The resulting DataFrame 'df' is sorted based on 'fltdep' in ascending order.</p>"},{"location":"Regression%20Models%20and%20Visualization/#section-3-machine-learning-model-training-and-evaluation","title":"Section 3: Machine Learning Model Training and Evaluation","text":"<p>This section focuses on training 341 XGBoost regression models, each corresponding to a specific number of days prior to the flight (count).</p>"},{"location":"Regression%20Models%20and%20Visualization/#training-and-saving-models","title":"Training and Saving Models","text":"<p>The code initiates a loop to create 341 models, each using a different subset of the data based on the 'count' value in the loop. For each iteration, a subset of data is prepared, and PCA (Principal Component Analysis) is applied for dimensionality reduction.</p> <p>The function then performs Principal Component Analysis (PCA) on the training features. The data is standardized using <code>StandardScaler</code>, and PCA is applied to reduce the dimensionality to 3 principal components:</p> <pre><code>scaler = StandardScaler()\nscaled_train_features = scaler.fit_transform(train_features)\n\nmodel = PCA(n_components=3).fit(scaled_train_features)\nX_pc = model.transform(scaled_train_features)\n</code></pre> <p>The data is split into training and testing sets, and an XGBoost regressor is trained with hyperparameters. The Mean Absolute Error (MAE) is calculated for each model based on the test data. The trained models are saved to separate pickle files for each flight and 'count' combination.</p>"},{"location":"Regression%20Models%20and%20Visualization/#section-4-multi-threading-for-model-training","title":"Section 4: Multi-threading for Model Training","text":"<p>The model training process is multi-threaded using <code>concurrent.futures.ThreadPoolExecutor</code>. It allows the function <code>train_model</code> to be executed concurrently for different <code>counts</code>, making the training process faster.</p>"},{"location":"Regression%20Models%20and%20Visualization/#section-5-visualization-of-predicted-sodp-on-calendar-heatmap","title":"Section 5: Visualization of Predicted 'SODP' on Calendar Heatmap","text":"<p>This section focuses on visualizing the predicted 'SODP' for the flight with flight number '0501' using a calendar heatmap with Plotly.</p>"},{"location":"Regression%20Models%20and%20Visualization/#calplot-of-predicted-sodp","title":"Calplot of Predicted 'SODP'","text":"<p>The code generates a calendar heatmap (calplot) of the predicted 'SODP' values for the flight '0501' in the year 2019. The color represents the predicted 'SODP' values.</p>"},{"location":"Regression%20Models%20and%20Visualization/#calplot-of-actual-sodp","title":"Calplot of Actual 'SODP'","text":"<p>Similarly, a calendar heatmap is generated to visualize the actual 'SODP' values for the flight '0501' in the year 2019.</p>"},{"location":"Regression%20Models%20and%20Visualization/#calplot-of-delta-sodp","title":"Calplot of Delta 'SODP'","text":"<p>Another calendar heatmap is generated to visualize the difference between the actual and predicted 'SODP' values for the flight '0501' in the year 2019.## Summary</p> <p>The provided Python script builds 341 regression models for a sample flight with flight number '0772'. It trains XGBoost regressors for each day prior to departure and visualizes the predictions using calendar heatmaps. The script serves as a data analysis and predictive modeling tool for flight data, helping to forecast 'SODP' values and evaluate model performance for different days prior to flight departure.</p>"}]}